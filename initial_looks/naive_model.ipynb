{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial naive modeling\n",
    "This is an effort to try creating a simple vectorfield to see if the discontinuities are plausible source locations. Each sensor location in the bounding box becomes a node with four vectors attached, one at each of the cardinal points of the compass. If the wind is blowing to the North, the vector attached to the north side of the node will be shown as pointing away from the node (an emitter) if the pollution is elevated; otherwise the vector will be shown pointing toward the node (a sink). In this scenario, we would expect to see this where P is a source of pollution and N are nodes:  \n",
    "<-- N <-- P --> N -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set bounding box\n",
    "#UP_LEFT = # (lat, lon)\n",
    "#UP_RIGHT = # (lat, lon)\n",
    "#DOWN_RIGHT = # (lat, lon)\n",
    "#DOWN_LEFT = # (lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set timeframe\n",
    "#START_DATE = # begin date to start taking data\n",
    "#END_DATE = # end date to start taking data\n",
    "#START_HOUR = # hour EACH DAY to start, this allows us to control for time of day effects\n",
    "#END_HOUR = # hour EACH DAY to end, this allows us to control for time of day effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUTURE BUILDOUT: get data function\n",
    "\n",
    "# month_df = get_data(UP_LEFT, UP_RIGHT, DOWN_RIGHT, DOWN_LEFT, START_DATE, END_DATE, START_HOUR, END_HOUR)\n",
    "# this call to get_data function that will take bounding box and timeframe and return cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, use one day of data from parquet file\n",
    "# grab sep27_full.parquet from the shared google drive\n",
    "\n",
    "# Ben's local path to the parquet file\n",
    "datafolder = \"../my_stash/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_parquet(\"{}/sep27_full.parquet\".format(datafolder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copied in Angshuman's data importing code now because my import wasn't working. I'll fix this soon. -Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Class for throwing custom errors\n",
    "class CustomError(Exception):\n",
    "    def __init__(self, m):\n",
    "        self.message = m\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "# Helper function for getting dates in a given range\n",
    "def getDates(start, end):\n",
    "    date_list = []\n",
    "    start_date = datetime.datetime.strptime(start, \"%Y/%m/%d\").date()\n",
    "    end_date = datetime.datetime.strptime(end, \"%Y/%m/%d\").date()\n",
    "\n",
    "    delta = end_date - start_date       # as timedelta\n",
    "\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        date_list.append(day.strftime(\"%Y%m%d\"))\n",
    "\n",
    "    return date_list\n",
    "\n",
    "# Helper function for loading data into a dataframe\n",
    "def loadDataframe(files):\n",
    "    df = pd.DataFrame(columns=['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0','pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime','rssi', \n",
    "                       'temperature', 'humidity', 'pm2_5_cf_1', 'a_h', 'device_loc_typ', 'high_reading_flag', 'hidden', 'sensor_id', 'sensor_name', 'lat', 'lon', 'parent_id', \n",
    "                       'is_owner', 'city', 'county', 'zipcode', 'created_at', 'year', 'month', 'day', 'hour', 'minute', 'wban_number', 'call_sign', 'call_sign2', 'interval', \n",
    "                       'call_sign3', 'zulu_time', 'report_modifier', 'wind_data', 'wind_direction', 'wind_speed', 'gusts', 'gust_speed', 'variable_winds', 'variable_wind_info', \n",
    "                       'sys_maint_reqd', 'epa_pm25_unit', 'epa_pm25_value', 'raw_concentration', 'aqi', 'category', 'site_name', 'agency_name', 'full_aqs_code', 'intl_aqs_code'])\n",
    "\n",
    "    for file in files:\n",
    "        file_name = \"{}.parquet\".format(file)\n",
    "        if path.exists(file_name):\n",
    "            tmp_df = pd.read_parquet(file_name)\n",
    "            df = pd.concat([df,tmp_df],ignore_index=True)\n",
    "        else:\n",
    "            print(\"File {} does not exist\".format(file_name))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Main function for getting data\n",
    "def get_data(UP_LEFT, UP_RIGHT, DOWN_RIGHT, DOWN_LEFT, START_DATE, END_DATE, START_HOUR, END_HOUR):\n",
    "\n",
    "    # Create variables from parameters\n",
    "    startfile = int(START_DATE.replace('/',''))\n",
    "    endfile = int(END_DATE.replace('/',''))\n",
    "    lat_min = DOWN_LEFT[0]\n",
    "    lat_max = UP_RIGHT[0]\n",
    "    lon_min = DOWN_LEFT[1]\n",
    "    lon_max = UP_RIGHT[1]\n",
    "\n",
    "    try:\n",
    "        if startfile <= endfile:\n",
    "            file_list = getDates(START_DATE, END_DATE)\n",
    "            df = loadDataframe(file_list)\n",
    "            # Filter data for input bounding box\n",
    "            df = df[(df.lat > lat_min) & (df.lat < lat_max) \n",
    "                              & (df.lon > lon_min) & (df.lon < lon_max)]\n",
    "            \n",
    "            # Filter data for input  hours\n",
    "            df = df[(df.hour >= START_HOUR) & (df.hour <= END_HOUR)]\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "            return df\n",
    "        else:\n",
    "            raise CustomError(\"INPUT ERROR: Start Date is greater than End Date\")\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '..datapurpleair20190927'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7d70d4aee1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mEND_HOUR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'18'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUP_LEFT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUP_RIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOWN_RIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOWN_LEFT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTART_DATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND_DATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTART_HOUR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND_HOUR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-22a6b41c0871>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(UP_LEFT, UP_RIGHT, DOWN_RIGHT, DOWN_LEFT, START_DATE, END_DATE, START_HOUR, END_HOUR)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Create variables from parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mstartfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_DATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mendfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEND_DATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mlat_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDOWN_LEFT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '..datapurpleair20190927'"
     ]
    }
   ],
   "source": [
    "# import sys, os\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'SingleDayAnalysis', 'getData'))\n",
    "# sys.path.append('..SingleDayAnalysis')\n",
    "# from getData import get_data\n",
    "\n",
    "# from ..SingleDayAnalysis/getData import get_data\n",
    "\n",
    "UP_LEFT = (38.008050, -122.536985)\n",
    "UP_RIGHT = (38.008050, -122.186437)\n",
    "DOWN_RIGHT = (37.701933, -122.186437)\n",
    "DOWN_LEFT = (37.701933, -122.536985)\n",
    "START_DATE = '2019/09/27'\n",
    "END_DATE = '2019/09/27'\n",
    "START_HOUR = '10'\n",
    "END_HOUR = '18'\n",
    "\n",
    "df = get_data(UP_LEFT, UP_RIGHT, DOWN_RIGHT, DOWN_LEFT, START_DATE, END_DATE, START_HOUR, END_HOUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with na data for 2_5um\n",
    "data_df = data_df[data_df['2_5um'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average counts by sensor\n",
    "average_col = data_df.groupby(['sensor_id'])['2_5um'].mean() \n",
    "data_df = data_df.set_index(['sensor_id'])\n",
    "data_df['avg_2_5um'] = average_col\n",
    "data_df = data_df.reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define direction degree range\n",
    "NORTH = (316,45)\n",
    "EAST = (46,135)\n",
    "SOUTH = (136,225)\n",
    "WEST = (226,315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through the dataframe and add new categorical column that indicates direction: \n",
    "# North, South, East, West, No wind, Missing, ERROR\n",
    "\n",
    "wind_compass = [] \n",
    "for row in range(len(data_df)):\n",
    "    try:\n",
    "        degree = int(data_df.loc[row].wind_direction)\n",
    "    except:\n",
    "        wind_compass.append('Missing')\n",
    "        continue\n",
    "    if data_df.loc[row].wind_speed == 0:\n",
    "        wind_compass.append('No wind')\n",
    "    elif degree >= NORTH[0] and degree <= NORTH[1]:\n",
    "        wind_compass.append('North')\n",
    "    elif degree >= EAST[0] and degree <= EAST[1]:\n",
    "        wind_compass.append('East')\n",
    "    elif degree >= SOUTH[0] and degree <= SOUTH[1]:\n",
    "        wind_compass.append('South')\n",
    "    elif degree >= WEST[0] and degree <= WEST[1]:\n",
    "        wind_compass.append('West')\n",
    "    else:\n",
    "        wind_compass.append('ERROR')\n",
    "data_df['wind_compass'] = wind_compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the counts. this is one day, so it may not show all possibilities.\n",
    "data_df.groupby('wind_compass').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average by compass counts\n",
    "\n",
    "# similar to above but need to do additional groupby on new categorical column 'wind_from_compass'\n",
    "# loop through each record, do a trig calculation based on angle to true direction, and sum up. \n",
    "# divide for avg and subtract the mean for the sensor from this average to get a +/- vector\n",
    "# add the five different vector values in a new column called 'vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the sensors on google maps api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the four vectors emerging from each direction off the sensor point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
