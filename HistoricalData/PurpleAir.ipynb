{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of PurpleAir's data\n",
    "[PurpleAir](http://www.purpleair.com) sells low-cost air quality sensors that feed data to [real-time maps of PM2.5 pollution](https://www.purpleair.com/map?#11/37.789/-122.2048).   \n",
    "This data will be used for a UC Berkeley capstone project [summarized here](https://docs.google.com/document/d/1NjCpqNd7rDnD6VOExVktGtquRzs21hpwZ8HhLQpYLO8/edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime, time\n",
    "from dateutil import tz\n",
    "import ast\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "import gmplot\n",
    "from math import floor\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "from fastparquet import ParquetFile, write\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to specify the paths for the data folder in your local machines\n",
    "# Use the variable 'datafolder' to specify the path\n",
    "# Comment out all the data paths except your own\n",
    "# Purple Air data ia assumed to be in a subfolder called 'purpleair' \n",
    "# For example, if the base data folder is '/users/data', purpleair data should be in '/users/data/purpleair'\n",
    "\n",
    "# Angshuman's local path\n",
    "datafolder = \"/Users/apaul2/Documents/_Common/capstone/Project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHashKey(row, col1, col2):\n",
    "    \n",
    "    str_col1 = row[col1]\n",
    "    \n",
    "    str_col2 = row[col2]\n",
    "        \n",
    "    return hash(str_col1 + str_col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate s3 files into daily raw data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is being pulled from the s3 bucket (midscapstone-whos-polluting-my-air) where we are storing all the purple air data files. The data files are being stored in a folder called 'PurpleAir' in this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "myopen = s3.open\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('midscapstone-whos-polluting-my-air')\n",
    "objs = bucket.objects.filter(Prefix='PurpleAir/2019')\n",
    "\n",
    "purple_df = pd.DataFrame(columns=['mapVersion', 'baseVersion', 'mapVersionString', 'results'])\n",
    "\n",
    "for obj in objs:\n",
    "    file_name = int(obj.key.replace('PurpleAir/2019','').replace('.parquet',''))\n",
    "    if file_name >= 9300659 and file_name < 10010659:\n",
    "        pf=ParquetFile('midscapstone-whos-polluting-my-air/{}'.format(obj.key), open_with=myopen)\n",
    "        df=pf.to_pandas()\n",
    "        purple_df = pd.concat([purple_df,df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    purple_df = pd.DataFrame.from_records(purple_df.results)\n",
    "except:\n",
    "    purple_df['results'] =  purple_df['results'].map(lambda d : ast.literal_eval(d))\n",
    "    purple_df = pd.DataFrame.from_records(purple_df.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purple_df['Stats'] = purple_df['Stats'].str.replace('15\"v3\":','15,\"v3\":')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dict in the 'Stats' column into separate columns\n",
    "purple_df['Stats'] = purple_df['Stats'].replace(np.nan, '{}', regex=True)\n",
    "purple_df['Stats'] =  purple_df['Stats'].map(lambda d : ast.literal_eval(d))\n",
    "purple_df = purple_df.join(pd.DataFrame(purple_df[\"Stats\"].to_dict()).T)\n",
    "purple_df.drop(['Stats', 'pm','v'], axis=1, inplace=True)   # 'pm' and 'v' are the same as 'PM2_5Value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_df = purple_df[['AGE', 'A_H', 'DEVICE_LOCATIONTYPE', 'Flag', 'Hidden', 'ID', 'Label',\n",
    "       'LastSeen', 'Lat', 'Lon', 'PM2_5Value', 'ParentID',\n",
    "       'THINGSPEAK_PRIMARY_ID', 'THINGSPEAK_PRIMARY_ID_READ_KEY',\n",
    "       'THINGSPEAK_SECONDARY_ID', 'THINGSPEAK_SECONDARY_ID_READ_KEY', 'Type',\n",
    "       'humidity', 'isOwner', 'pressure', 'temp_f', 'lastModified',\n",
    "       'timeSinceModified', 'v1', 'v2', 'v3', 'v4', 'v5',\n",
    "       'v6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE', 'A_H', 'DEVICE_LOCATIONTYPE', 'Flag', 'Hidden', 'ID', 'Label',\n",
       "       'LastSeen', 'Lat', 'Lon', 'PM2_5Value', 'ParentID',\n",
       "       'THINGSPEAK_PRIMARY_ID', 'THINGSPEAK_PRIMARY_ID_READ_KEY',\n",
       "       'THINGSPEAK_SECONDARY_ID', 'THINGSPEAK_SECONDARY_ID_READ_KEY', 'Type',\n",
       "       'humidity', 'isOwner', 'pressure', 'temp_f', 'lastModified',\n",
       "       'timeSinceModified', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purple_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purple_df.drop(['Ozone1','Voc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to something easily understandable\n",
    "purple_df.columns = ['age','a_h','device_loc_typ','high_reading_flag', 'hidden','sensor_id','sensor_name','last_seen','lat','lon',\n",
    "                      'pm2_5val','parent_id','thingspeak_primary_id','thingspeak_primary_id_read_key','thingspeak_secondary_id',\n",
    "                      'thingspeak_secondary_id_read_key','sensor_type','humidity','is_owner','pressure','temp_f','av_stat_last_modified',\n",
    "                      'av_stat_time_since_last_modified','pm2_5val_10m_avg','pm2_5val_30m_avg','pm2_5val_1h_avg','pm2_5val_6h_avg',\n",
    "                      'pm2_5val_24h_avg','pm2_5val_1wk_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "parquet_file = \"{}/purpleair/dailyraw/20190916.parquet\".format(datafolder)\n",
    "write(parquet_file, purple_df,compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get only required columns from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>baseVersion</th>\n",
       "      <th>mapVersion</th>\n",
       "      <th>mapVersionString</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ID': 24115, 'Label': ' 2nd South 12th East',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ID': 24116, 'ParentID': 24115, 'Label': ' 2n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ID': 27699, 'Label': ' CHA1', 'DEVICE_LOCATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ID': 27700, 'ParentID': 27699, 'Label': ' CH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ID': 16791, 'Label': ' DW0435', 'DEVICE_LOCA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  baseVersion  mapVersion  mapVersionString  \\\n",
       "0           0            6        0.88               NaN   \n",
       "1           1            6        0.88               NaN   \n",
       "2           2            6        0.88               NaN   \n",
       "3           3            6        0.88               NaN   \n",
       "4           4            6        0.88               NaN   \n",
       "\n",
       "                                             results  \n",
       "0  {'ID': 24115, 'Label': ' 2nd South 12th East',...  \n",
       "1  {'ID': 24116, 'ParentID': 24115, 'Label': ' 2n...  \n",
       "2  {'ID': 27699, 'Label': ' CHA1', 'DEVICE_LOCATI...  \n",
       "3  {'ID': 27700, 'ParentID': 27699, 'Label': ' CH...  \n",
       "4  {'ID': 16791, 'Label': ' DW0435', 'DEVICE_LOCA...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purple_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14,17):\n",
    "    purple_df = pd.read_parquet(\"{}/purpleair/dailyraw/201909{}.parquet\".format(datafolder,i))\n",
    "    # Drop unwanted columns\n",
    "    purple_df.drop(['age','av_stat_last_modified', 'av_stat_time_since_last_modified','pm2_5val_10m_avg', 'pm2_5val_30m_avg', 'pm2_5val_1h_avg',\n",
    "           'pm2_5val_6h_avg', 'pm2_5val_24h_avg', 'pm2_5val_1wk_avg', 'pm2_5val','humidity','pressure','temp_f','sensor_type'], axis=1, inplace=True)\n",
    "    # There may be duplicates in sensor data in case no new readings we obtained since the last refresh\n",
    "    purple_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    bayarea_purple_df = purple_df[(purple_df.lat > 37.701933) & (purple_df.lat < 38.008050) \n",
    "                              & (purple_df.lon > -122.536985) & (purple_df.lon < -122.186437)]\n",
    "    bayarea_purple_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Get date and time columns in local timezone\n",
    "    bayarea_purple_df['year'] = bayarea_purple_df['last_seen'].apply(lambda x: datetime.datetime.fromtimestamp(x).replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).strftime(\"%Y\"))\n",
    "    bayarea_purple_df['month'] = bayarea_purple_df['last_seen'].apply(lambda x: datetime.datetime.fromtimestamp(x).replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).strftime(\"%m\"))\n",
    "    bayarea_purple_df['day'] = bayarea_purple_df['last_seen'].apply(lambda x: datetime.datetime.fromtimestamp(x).replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).strftime(\"%d\"))\n",
    "    bayarea_purple_df['hour'] = bayarea_purple_df['last_seen'].apply(lambda x: datetime.datetime.fromtimestamp(x).replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).strftime(\"%H\"))\n",
    "    bayarea_purple_df['minute'] = bayarea_purple_df['last_seen'].apply(lambda x: datetime.datetime.fromtimestamp(x).replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).strftime(\"%M\"))\n",
    "    bayarea_purple_df['10min'] = bayarea_purple_df['minute'].apply(lambda x: \"{:02}\".format(10 * floor(int(x)/10)))\n",
    "    \n",
    "    bayarea_purple_df['datetime'] = bayarea_purple_df[['year', 'month','day','hour','10min']].apply(lambda x: int(''.join(x)), axis=1)\n",
    "    \n",
    "    # Drop unwanted columns from purple air data\n",
    "    bayarea_purple_df.drop(['last_seen', 'hour', 'minute', '10min'], axis = 1, inplace=True)\n",
    "    bayarea_purple_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Write to file\n",
    "    parquet_file = \"{}/purpleair/dailyfull/201909{}.parquet\".format(datafolder,i)\n",
    "    write(parquet_file, bayarea_purple_df,compression='GZIP')\n",
    "    \n",
    "    bayarea_purple_dly_df =bayarea_purple_df[(bayarea_purple_df.year == '2019') & (bayarea_purple_df.month == '09') & (bayarea_purple_df.day == str(i))]\n",
    "    \n",
    "    # Drop unwanted columns from purple air data\n",
    "    bayarea_purple_dly_df.drop(['year', 'month', 'day'], axis = 1, inplace=True)\n",
    "    bayarea_purple_dly_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Add hash column based on the primary and secondary keys\n",
    "    bayarea_purple_dly_df['sensorhash'] = bayarea_purple_dly_df.apply (lambda row: createHashKey(row,'thingspeak_primary_id_read_key',\n",
    "                                                                                                    'thingspeak_secondary_id_read_key'), axis=1)\n",
    "    # Write to file\n",
    "    parquet_file = \"{}/purpleair/dailyfiltered/201909{}.parquet\".format(datafolder,i)\n",
    "    write(parquet_file, bayarea_purple_dly_df,compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58896, 294, 294, 587)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayarea_purple_dly_df.sensor_id.count(), bayarea_purple_dly_df.lat.nunique(), bayarea_purple_dly_df.lon.nunique(), bayarea_purple_dly_df.sensorhash.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pd.read_parquet(\"{}/purpleair/dailyfiltered/20190930.parquet\".format(datafolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201909300000, 201909300010, 201909300020, 201909300030,\n",
       "       201909300040, 201909300050, 201909300100, 201909300110,\n",
       "       201909300120, 201909300130, 201909300140, 201909300150,\n",
       "       201909300200, 201909300210, 201909300220, 201909300230,\n",
       "       201909300240, 201909300250, 201909300300, 201909300310,\n",
       "       201909300320, 201909300330, 201909300340, 201909300350,\n",
       "       201909300400, 201909300410, 201909300420, 201909300430,\n",
       "       201909300440, 201909300450, 201909300500, 201909300510,\n",
       "       201909300520, 201909300530, 201909300540, 201909300550,\n",
       "       201909300600, 201909300610, 201909300620, 201909300630,\n",
       "       201909300640, 201909300650, 201909300700, 201909300710,\n",
       "       201909300720, 201909300730, 201909300740, 201909300750,\n",
       "       201909300800, 201909300810, 201909300820, 201909300830,\n",
       "       201909300840, 201909300850, 201909300900, 201909300910,\n",
       "       201909300920, 201909300930, 201909300940, 201909300950,\n",
       "       201909301000, 201909301010, 201909301020, 201909301030,\n",
       "       201909301040, 201909301050, 201909301100, 201909301110,\n",
       "       201909301120, 201909301130, 201909301140, 201909301150,\n",
       "       201909301200, 201909301210, 201909301220, 201909301230,\n",
       "       201909301240, 201909301250, 201909301300, 201909301310,\n",
       "       201909301320, 201909301330, 201909301340, 201909301350,\n",
       "       201909301400, 201909301410, 201909301420, 201909301430,\n",
       "       201909301440, 201909301450, 201909301500, 201909301510,\n",
       "       201909301520, 201909301530, 201909301540, 201909301550,\n",
       "       201909301600, 201909301610, 201909301620, 201909301630,\n",
       "       201909301640, 201909301650])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.datetime.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
