{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of PurpleAir's data\n",
    "[PurpleAir](http://www.purpleair.com) sells low-cost air quality sensors that feed data to [real-time maps of PM2.5 pollution](https://www.purpleair.com/map?#11/37.789/-122.2048).   \n",
    "This data will be used for a UC Berkeley capstone project [summarized here](https://docs.google.com/document/d/1NjCpqNd7rDnD6VOExVktGtquRzs21hpwZ8HhLQpYLO8/edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import datetime, time\n",
    "from dateutil import tz\n",
    "import ast\n",
    "import re\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "import gmplot\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "from fastparquet import ParquetFile, write\n",
    "\n",
    "import urllib3\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "https = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to specify the paths for the data folder in your local machines\n",
    "# Use the variable 'datafolder' to specify the path\n",
    "# Comment out all the data paths except your own\n",
    "# Purple Air data ia assumed to be in a subfolder called 'purpleair' \n",
    "# For example, if the base data folder is '/users/data', purpleair data should be in '/users/data/purpleair'\n",
    "\n",
    "# Angshuman's local path\n",
    "datafolder = \"/Users/apaul2/Documents/_Common/capstone/Project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHashKey(row):\n",
    "    if np.isnan(row['lat']):\n",
    "        str_lat = ''\n",
    "    else:\n",
    "        str_lat = str(row['lat'])\n",
    "        \n",
    "        \n",
    "    if np.isnan(row['lon']):\n",
    "        str_lon = ''\n",
    "    else:\n",
    "        str_lon = str(row['lon'])\n",
    "        \n",
    "    return hash(str_lat + str_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from sensor 2\n",
    "def genTS2DF(sensordf, month, startday, yr):\n",
    "    ts_s_df = pd.DataFrame(columns=['created_at', '0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0', 'pm10_0','sensorhash'])\n",
    "    count, errCount = 0, 0\n",
    "\n",
    "    for ind, val in sensordf.iterrows():\n",
    "        qrystr = \"https://api.thingspeak.com/channels/{0}/feeds.json?api_key={1}&start=20{4}-{2}-{3}%2000:00:00&end=20{4}-{2}-{3}%2023:59:59& \\\n",
    "                    timezone=America/Los_Angeles&timescale=10\".format(val['thingspeak_secondary_id'], val['thingspeak_secondary_id_read_key'], month, startday, yr)\n",
    "#         print(qrystr)\n",
    "        try:\n",
    "            count += 1\n",
    "            r = https.request('GET',qrystr)\n",
    "            if r.status == 200:\n",
    "                j = json.loads(r.data.decode('utf-8'))\n",
    "                df = pd.DataFrame(j['feeds'])\n",
    "                df.columns=['created_at', '0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0', 'pm10_0']\n",
    "                df['sensorhash'] = val['sensorhash']\n",
    "                ts_s_df = pd.concat([ts_s_df,df],ignore_index=True)\n",
    "        except Exception as e:\n",
    "            errCount += 1\n",
    "            continue\n",
    "    print(\"For {}, Of the {} requests, {} errored out.\".format(startday, count, errCount))\n",
    "    \n",
    "    # Add a key column based on time\n",
    "    # This along with the sensorhash column will be used to join the two sensor datasets\n",
    "    ts_s_df['created'] = ts_s_df['created_at'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y%m%d%H%M\"))\n",
    "    \n",
    "    return ts_s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from sensor 1\n",
    "def genTS1DF(sensordf, month, startday, yr):\n",
    "    ts_p_df = pd.DataFrame(columns=['created_at', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime', 'rssi', 'temperature', 'humidity', 'pm2_5_cf_1','sensorhash'])\n",
    "    count, errCount = 0, 0\n",
    "\n",
    "    for ind, val in sensordf.iterrows():\n",
    "        qrystr = \"https://api.thingspeak.com/channels/{0}/feeds.json?api_key={1}&start=20{4}-{2}-{3}%2000:00:00&end=20{4}-{2}-{3}%2023:59:59& \\\n",
    "                    timezone=America/Los_Angeles&timescale=10\".format(val['thingspeak_primary_id'], val['thingspeak_primary_id_read_key'], month, startday, yr)\n",
    "#         print(qrystr)\n",
    "        try:\n",
    "            count += 1\n",
    "            r = https.request('GET',qrystr)\n",
    "            if r.status == 200:\n",
    "                j = json.loads(r.data.decode('utf-8'))\n",
    "                df = pd.DataFrame(j['feeds'])\n",
    "                df.columns=['created_at', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime', 'rssi', 'temperature', 'humidity', 'pm2_5_cf_1']\n",
    "                df['sensorhash'] = val['sensorhash']\n",
    "                ts_p_df = pd.concat([ts_p_df,df],ignore_index=True)\n",
    "        except Exception as e:\n",
    "            errCount += 1\n",
    "            continue\n",
    "    print(\"Of the {} requests, {} errored out.\".format(count, errCount))\n",
    "    \n",
    "    # Add a key column based on time\n",
    "    # This along with the sensorhash column will be used to join the two sensor datasets\n",
    "    ts_p_df['created'] = ts_p_df['created_at'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y%m%d%H%M\"))\n",
    "    \n",
    "    return ts_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get thingspeak data:\n",
    "def getThingspeakData(bayarea_purple_df, month, monthname, day, yr):\n",
    "    bay_pa_thingspeak_df = bayarea_purple_df[['sensorhash', 'thingspeak_primary_id','thingspeak_primary_id_read_key',\n",
    "                                               'thingspeak_secondary_id','thingspeak_secondary_id_read_key']]\n",
    "    bay_pa_thingspeak_df.drop_duplicates(inplace=True)\n",
    "    bay_pa_thingspeak_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    ts_s_df = genTS2DF(bay_pa_thingspeak_df, month, \"{:02}\".format(day), yr)\n",
    "    ts_p_df = genTS1DF(bay_pa_thingspeak_df, month, \"{:02}\".format(day), yr)\n",
    "    # Merge data from the two sensors\n",
    "    # Only keep records having particle data\n",
    "    bay_ts_df = pd.merge(ts_s_df, ts_p_df,  how='left', left_on=['sensorhash','created'], right_on=['sensorhash','created'])\n",
    "    bay_ts_df.drop(['created_at_y'], axis=1, inplace=True)\n",
    "    \n",
    "    # Write to file\n",
    "    parquet_file = \"{}/thingspeak/thingspeak_{}{:02}.parquet\".format(datafolder, monthname, day)\n",
    "    write(parquet_file, bay_ts_df,compression='GZIP')\n",
    "    \n",
    "    return bay_ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Purple Air data\n",
    "def mergePurpleAir(pa_df, ts_df, address_df, month, day, yr):\n",
    "    # Some numeric columns may have \"nan\" as a string - convert these values to np.nan\n",
    "    # so that the data type of these columns are correctly identified\n",
    "    ts_df[['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0','pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime',\n",
    "           'rssi', 'temperature', 'humidity', 'pm2_5_cf_1']] = ts_df[['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0',\n",
    "           'pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime', 'rssi', 'temperature', 'humidity', 'pm2_5_cf_1']].replace(\"nan\", np.nan, regex=True)\n",
    "    ts_df[['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0','pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime',\n",
    "           'rssi', 'temperature', 'humidity', 'pm2_5_cf_1']] = ts_df[['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0',\n",
    "           'pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime', 'rssi', 'temperature', 'humidity', 'pm2_5_cf_1']].apply(pd.to_numeric)\n",
    "    \n",
    "    # Merge purple air data with sensor data\n",
    "    # Only keep records having particle data\n",
    "    ts_df = pd.merge(ts_df, pa_df,  how='left', left_on=['sensorhash'], right_on=['sensorhash'])\n",
    "    \n",
    "    # Join address dataframe with main dataframe\n",
    "    ts_df = pd.merge(ts_df, address_df,  how='left', left_on=['lat','lon'], right_on=['lat','lon'])\n",
    "    \n",
    "    ts_df['created_at'] = ts_df['created_at_x'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%dT%H:%M\"))\n",
    "    ts_df['year'] = ts_df['created_at_x'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y\"))\n",
    "    ts_df['month'] = ts_df['created_at_x'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%m\"))\n",
    "    ts_df['day'] = ts_df['created_at_x'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%d\"))\n",
    "    ts_df['hour'] = ts_df['created_at_x'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%H\"))\n",
    "    ts_df['minute'] = ts_df['created_at_x'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%M\"))\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    ts_df.drop(['created_at_x', 'sensorhash', 'country','state'], axis = 1, inplace=True)\n",
    "\n",
    "# Add nan values for purple air data as we dont have data for these elements prior to 09/14\n",
    "    ts_df['a_h'] = None\n",
    "    ts_df['high_reading_flag'] = np.nan\n",
    "    ts_df['hidden'] = None\n",
    "\n",
    "    # Convert data type of attributes to string\n",
    "    ts_df[['high_reading_flag','sensor_id','parent_id', 'is_owner']] = ts_df[['high_reading_flag','sensor_id','parent_id', 'is_owner']].astype(str)\n",
    "    \n",
    "    # Save final dataframe for future use\n",
    "#     parquet_file = \"{}/pa_ts/201909{}.parquet\".format(datafolder,days_list[i])\n",
    "    parquet_file = \"{}/pa_ts/20{}{}{:02}.parquet\".format(datafolder, yr, month, day)\n",
    "    write(parquet_file, ts_df,compression='GZIP')\n",
    "    \n",
    "    return ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNOAAdf(lines, fileName):\n",
    "    \"\"\" Helper function to process noaa data\"\"\"\n",
    "    \n",
    "    # split lines and data chunks\n",
    "    data = [] # an array of arrays, inner arrays are all data for one record, outer array is all records\n",
    "    for line in lines:\n",
    "\n",
    "        # reset any variables if needed\n",
    "        record = [] \n",
    "        Report_Modifier = ''\n",
    "        Wind_Data = False \n",
    "        Variable_Winds = False\n",
    "        Gusts = False\n",
    "        Wind_Direction = ''\n",
    "        Wind_Speed = ''\n",
    "        Gust_Speed = ''\n",
    "        Variable_Wind_Info = ''\n",
    "        System_Maintenance_Reqd = False\n",
    "\n",
    "        try:\n",
    "            line = line.split() # take string of one record's data and split into space separated chunks\n",
    "            WBAN_Number = line[0][0:5] # The WBAN (Weather Bureau, Army, Navy) number is a unique 5-digit number\n",
    "            Call_Sign = line[0][5:] # The call sign is a location identifier, three or four characters in length \n",
    "            suffix = line[1][-2:] # grab the last two digits that are the year (i.e. 19 for 2019)\n",
    "            Year = '20'+suffix # in YYYY format\n",
    "            CallSign_Date = re.split(Year, line[1])\n",
    "            Call_Sign2 = CallSign_Date[0] # this seems to be the same as Call_Sign but without initial letter\n",
    "            Date = CallSign_Date[1]\n",
    "            Month = Date[0:2] # in MM format\n",
    "            Day = Date[2:4] # in DD format\n",
    "            Hour = Date[4:6] # in HH format\n",
    "            Minute = Date[6:8] # Observations are recorded on whole five-minute increments (i.e. 00,05,10,...,50,55)\n",
    "            Record_Length = Date[8:11] # I'm not sure what this is yet - Length of record??\n",
    "            Date = Date[11:] # MM/DD/YY format\n",
    "            Timestamp = line[2] # in HH:MM:SS format\n",
    "            Interval = line[3] # should be 5-MIN as opposed to 1-MIN\n",
    "            Call_Sign3 = line[4] # for some reason, a THIRD output of the call sign. random.\n",
    "            Zulu_Time = line[5] # Zulu Time, or military time, or UTC\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # after this point, data could be missing/optional and data positions are not fixed\n",
    "        currIndx = 6\n",
    "        try:\n",
    "            Next_Data = line[currIndx]\n",
    "            if not any(x in Next_Data for x in ['KT','SM']):\n",
    "                Report_Modifier = Next_Data # AUTO for fully automated report, COR for correction to a previously disseminated report\n",
    "                currIndx += 1\n",
    "            Next_Data = line[currIndx]\n",
    "            if \"KT\" in Next_Data:\n",
    "                Wind_Data = True\n",
    "                Wind_Direction = Next_Data[0:3] # in tens of degrees from true north\n",
    "                if Next_Data[0:3] == 'VRB':\n",
    "                    Variable_Winds = True\n",
    "                Wind_Speed = Next_Data[3:5] # in whole knots (two digits)\n",
    "                if Next_Data[5] == 'G':\n",
    "                    Gusts = True\n",
    "                    Gust_Speed = Next_Data[6:8] # speed in whole knots (two digits)\n",
    "            else:\n",
    "                Wind_Data = False\n",
    "        except:\n",
    "            print(\"OUT OF DATA AT FIELD {}\".format(currIndx))\n",
    "            print(line)\n",
    "        finally:\n",
    "            currIndx += 1\n",
    "\n",
    "        try:\n",
    "            Next_Data = line[currIndx]\n",
    "            if Wind_Data:\n",
    "                if (re.fullmatch(r'[0-9][0-9][0-9]V[0-9][0-9][0-9]', Next_Data)): #e.g. 180V240 = wind direction varies from 180 to 240 degrees\n",
    "                    Variable_Wind_Info = Next_Data\n",
    "                    Variable_Winds = True\n",
    "        except:\n",
    "            print(\"OUT OF DATA AT FIELD {}\".format(currIndx))\n",
    "            print(line)\n",
    "            \n",
    "        if line[-1] == '$':\n",
    "            System_Maintenance_Reqd = True\n",
    "\n",
    "        #Sea_Level_Pressure = line[13] # given in tenths of hectopascals (millibars). The last digits are recorded (125 means 1012.5)\n",
    "        #Station_Type = line[18]\n",
    "        Num_Fields = len(line)\n",
    "        record = [WBAN_Number, Call_Sign, Call_Sign2, Year, Month, Day, Hour, Minute, Record_Length, Date, Timestamp, Interval, Call_Sign3, Zulu_Time, \n",
    "                  Report_Modifier, Wind_Data, Wind_Direction, Wind_Speed, Gusts, Gust_Speed, Variable_Winds, Variable_Wind_Info, System_Maintenance_Reqd, Num_Fields]\n",
    "        col_names = [\"wban_number\", \"call_sign\", \"call_sign2\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"rec_length\", \"date\", \"timestamp\", \"interval\", \"call_sign3\", \n",
    "                     \"zulu_time\", \"report_modifier\", \"wind_data\", \"wind_direction\", \"wind_speed\", \"gusts\", \"gust_speed\", \"variable_winds\", \"variable_wind_info\", \"sys_maint_reqd\", \"num_fields\"]\n",
    "        data.append(record)\n",
    "    \n",
    "    sample_df = pd.DataFrame(data, columns = col_names)\n",
    "    \n",
    "    # save Dataframe to file\n",
    "    parquet_file = \"{}/noaa/{}.parquet\".format(datafolder, fileName)\n",
    "    write(parquet_file, sample_df,compression='GZIP')\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get noaa data for the month\n",
    "def getNOAAData(month, monthyear, yr):\n",
    "    # Read station data from file that was stored earlier\n",
    "    unique_station_df = pd.read_parquet(\"{}/noaa/uniq_station_data.parquet\".format(datafolder))\n",
    "\n",
    "    # List of NOAA stations in the 35 < lat < 40 and  -125 < lon < -120 bounding box\n",
    "    station_list = ['KAPC', 'KBLU', 'KCCR', 'KHWD', 'KLVK', 'KMAE', 'KMCE', 'KMOD', 'KMRY', 'KMYV', 'KNUQ', 'KOAK', 'KOVE', 'KPRB', 'KSAC', 'KSBP', 'KSCK', \n",
    "                    'KSFO', 'KSJC', 'KSMF', 'KSNS', 'KSTS', 'KUKI', 'KVCB', 'KWVI']\n",
    "\n",
    "    # Get NOAA data for desired stattions in a list\n",
    "    lines = [] # an array of each read line\n",
    "    for station in station_list:\n",
    "        filepath = \"ftp://ftp.ncdc.noaa.gov/pub/data/asos-fivemin/6401-20{2}/64010{0}20{2}{1}.dat\".format(station, month, yr)\n",
    "        try:\n",
    "            for line in pd.read_csv(filepath_or_buffer=filepath , encoding='utf-8', header=None, chunksize=1):\n",
    "                lines.append(line.iloc[0,0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create noaa dataframe for the month\n",
    "    noaa_df = createNOAAdf(lines, monthyear)\n",
    "    # Drop rows where wind speed is not numeric\n",
    "    noaa_df = noaa_df[noaa_df.wind_speed != 'T']\n",
    "    merged_noaa_df = pd.merge(noaa_df, unique_station_df, on='wban_number')\n",
    "    # Convert data type of numeric columns\n",
    "    merged_noaa_df[['wind_speed','gust_speed','lat','lon']] = merged_noaa_df[['wind_speed','gust_speed','lat','lon']].apply(pd.to_numeric)\n",
    "\n",
    "    # Get data for bounding box\n",
    "    bay_noaa_df = merged_noaa_df[(merged_noaa_df.lat > 35) & (merged_noaa_df.lat < 40) \n",
    "                                  & (merged_noaa_df.lon > -125) & (merged_noaa_df.lon < -120)]\n",
    "    bay_noaa_df.reset_index(inplace=True, drop=True)\n",
    "    bay_noaa_df['datetime'] = bay_noaa_df[['year', 'month','day','hour','minute']].apply(lambda x: int(''.join(x)), axis=1)\n",
    "\n",
    "    return bay_noaa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily noaa data for the given month\n",
    "def getDailyNOAA(bay_noaa_df, month, day, yr):\n",
    "    datestr = '{}/{:02}/{}'.format(month, day, yr)\n",
    "    dly_noaa_df = bay_noaa_df[bay_noaa_df.date == datestr]\n",
    "    dly_noaa_df.drop(['year', 'month','day','hour','minute','date','timestamp'], axis=1, inplace=True)\n",
    "    parquet_file = \"{0}/noaa/daily/asos_20{3}{1}{2:02}.parquet\".format(datafolder, month, day, yr)\n",
    "    write(parquet_file, dly_noaa_df,compression='GZIP')\n",
    "    \n",
    "    return dly_noaa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get epa data\n",
    "def getEPAHistData():\n",
    "    epa_df = pd.read_csv(\"{}/ambient/historical_PM25.csv\".format(datafolder))\n",
    "    epa_df.columns = ['lat', 'lon', 'utc', 'parameter', 'epa_pm25_unit', 'epa_pm25_value','raw_concentration', 'aqi', 'category', 'site_name', 'agency_name',\n",
    "       'full_aqs_code', 'intl_aqs_code']\n",
    "    \n",
    "    # Add a datekey column based on local date\n",
    "    epa_df['created'] = epa_df['utc'].apply(lambda x: int(datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').replace(tzinfo=tz.tzutc()).astimezone(tz.tzlocal()).strftime(\"%Y%m%d%H%M\")))\n",
    "    \n",
    "    return epa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily interpolated epa data\n",
    "def getEPADailyData(dateint, dt_ind, month, epa_df, yr):\n",
    "    start = dateint + dt_ind * 10000\n",
    "    end = start + 10001\n",
    "    dly_epa_df = epa_df[(epa_df.created >= start) & (epa_df.created < end)]\n",
    "    dly_epa_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    new_df = pd.DataFrame(columns=['lat', 'lon', 'utc', 'parameter', 'epa_pm25_unit', 'epa_pm25_value', 'raw_concentration', 'aqi', 'category', 'site_name', 'agency_name', 'full_aqs_code', 'intl_aqs_code', 'created'])\n",
    "    for sitenm in dly_epa_df.site_name.unique():\n",
    "        indx_ct = 0\n",
    "        site_df = dly_epa_df[dly_epa_df.site_name == sitenm]\n",
    "        for i in site_df.created.unique():\n",
    "            indx_ct += 1\n",
    "            new_df =  pd.concat([new_df,site_df.iloc[indx_ct - 1:indx_ct]],ignore_index=True)\n",
    "\n",
    "            if i != site_df.created.max(): # Don't interpolate the last record\n",
    "                tmp_df = site_df.iloc[indx_ct - 1:indx_ct][['lat', 'lon', 'utc', 'parameter', 'epa_pm25_unit', 'category', 'site_name', 'agency_name', 'full_aqs_code', 'intl_aqs_code']]\n",
    "                for j in range(1,6):\n",
    "                    new_dt = i + j * 10\n",
    "            #         print(indx_start, j, new_dt)\n",
    "                    tmp_df['created'] = int(new_dt)\n",
    "                    tmp_df['epa_pm25_value'] = np.nan\n",
    "                    tmp_df['raw_concentration'] = np.nan\n",
    "                    tmp_df['aqi'] = np.nan\n",
    "                    new_df =  pd.concat([new_df,tmp_df],ignore_index=True)\n",
    "\n",
    "    # Convert aqi to numerica for so that it gets interpolated\n",
    "    new_df[['aqi']] = new_df[['aqi']].replace(\"nan\", np.nan, regex=True)\n",
    "    new_df[['aqi']] = new_df[['aqi']].apply(pd.to_numeric)\n",
    "\n",
    "    new_df = new_df.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "\n",
    "    int_epa_df = new_df[(new_df.created >= start) & (new_df.created < (end - 1))]\n",
    "    int_epa_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    parquet_file = \"{0}/ambient/daily_interpolated/epa_20{3}{1}{2:02}.parquet\".format(datafolder, month, dt_ind, yr)\n",
    "    write(parquet_file, int_epa_df,compression='GZIP')\n",
    "    \n",
    "    return int_epa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for mapping closest lat-lon data point\n",
    "def mapLatLon(ts_df, ts_latlon_df, lkp_df, maphashcol, datecol):\n",
    "    # Add lat-lon based hashes to noaa and purple air dataframes\n",
    "    lkp_df[maphashcol] = lkp_df.apply (lambda row: createHashKey(row), axis=1)\n",
    "    \n",
    "    # Keep only the asos columns needed to determine the lat-lon mapping\n",
    "    lkp_latlon_df = lkp_df[[maphashcol,'lat','lon']]\n",
    "    lkp_latlon_df.drop_duplicates(inplace=True)\n",
    "    lkp_latlon_df.set_index(maphashcol, inplace=True)\n",
    "    \n",
    "    # Find the closest lat-lon mapping corresponding to the purple air records\n",
    "    closest_points = {}\n",
    "    for name, point in ts_latlon_df.iterrows():\n",
    "    #     print(name, point)\n",
    "    #     break\n",
    "        distances = (((lkp_latlon_df - point) ** 2).sum(axis=1)**.5)\n",
    "        closest_points[name] = distances.sort_values().index[0]\n",
    "\n",
    "    # Create dataframe from lat-lon mapping\n",
    "    latlonmap_df = pd.DataFrame(list(closest_points.items()), columns=['tslatlonhash',maphashcol])\n",
    "    \n",
    "    # Merge purple air data to lat-lon mappings first and then \n",
    "    # merge the resulting dataframe to asos and epa dataframes\n",
    "    merged_df = pd.merge(ts_df, latlonmap_df, on='tslatlonhash')\n",
    "    \n",
    "     # Drop common and unwanted columns from noaa and epa dataframes\n",
    "    lkp_df.drop(['lat','lon'], axis=1, inplace=True)\n",
    "    \n",
    "    # Combine asos data\n",
    "    combined_df = pd.merge(merged_df, lkp_df,  how='left', left_on=[maphashcol, 'created'], right_on=[maphashcol, datecol])\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine data from various sources\n",
    "def combineData(noaa_df, epa_df, bay_ts_df, month, day, yr):\n",
    "    # Add lat-lon based hashes to noaa and purple air dataframes\n",
    "    bay_ts_df['tslatlonhash'] = bay_ts_df.apply (lambda row: createHashKey(row), axis=1)\n",
    "    \n",
    "    # Keep only the purple air columns needed to determine the lat-lon mapping\n",
    "    ts_latlon_df = bay_ts_df[['tslatlonhash','lat','lon']]\n",
    "    ts_latlon_df.drop_duplicates(inplace=True)\n",
    "    ts_latlon_df.set_index('tslatlonhash', inplace=True)\n",
    "    \n",
    "    # Find the closest asos lat-lon mapping corresponding to the purple air records\n",
    "    combined_df = mapLatLon(bay_ts_df, ts_latlon_df, noaa_df, 'asoslatlonhash', 'datetime')\n",
    "     \n",
    "    # Find the closest asos lat-lon mapping corresponding to the purple air records\n",
    "    combined_df = mapLatLon(combined_df, ts_latlon_df, epa_df, 'epalatlonhash', 'created')\n",
    "     \n",
    "    # Drop unwanted columns\n",
    "    combined_df.drop(['tslatlonhash', 'asoslatlonhash', 'epalatlonhash', 'rec_length','num_fields', 'datetime', 'utc', 'parameter'], axis=1, inplace=True)\n",
    "\n",
    "    combined_df.columns = ['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0', 'pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime','rssi', 'temperature', 'humidity', 'pm2_5_cf_1', \n",
    "                       'device_loc_typ','is_owner', 'sensor_id', 'sensor_name', 'parent_id', 'lat', 'lon','thingspeak_primary_id', 'thingspeak_primary_id_read_key','thingspeak_secondary_id', \n",
    "                       'thingspeak_secondary_id_read_key', 'city', 'county', 'zipcode', 'created_at', 'year', 'month', 'day', 'hour', 'minute', 'a_h', 'high_reading_flag', 'hidden', 'wban_number', \n",
    "                       'call_sign', 'call_sign2', 'interval', 'call_sign3', 'zulu_time', 'report_modifier', 'wind_data', 'wind_direction', 'wind_speed', 'gusts','gust_speed', 'variable_winds', 'variable_wind_info', \n",
    "                       'sys_maint_reqd', 'agency_name', 'aqi', 'category', 'epa_pm25_unit', 'epa_pm25_value', 'full_aqs_code', 'intl_aqs_code', 'raw_concentration', 'site_name']\n",
    "    \n",
    "    combined_df = combined_df[['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0','pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime','rssi', \n",
    "                       'temperature', 'humidity', 'pm2_5_cf_1', 'device_loc_typ', 'is_owner', 'sensor_id', 'sensor_name', 'parent_id','lat', 'lon',  'thingspeak_primary_id', \n",
    "                       'thingspeak_primary_id_read_key', 'thingspeak_secondary_id', 'thingspeak_secondary_id_read_key', 'a_h', 'high_reading_flag', 'hidden',\n",
    "                       'city', 'county', 'zipcode', 'created_at', 'year', 'month', 'day', 'hour', 'minute', 'wban_number', 'call_sign', 'call_sign2', 'interval', \n",
    "                       'call_sign3', 'zulu_time', 'report_modifier', 'wind_data', 'wind_direction', 'wind_speed', 'gusts', 'gust_speed', 'variable_winds', 'variable_wind_info', \n",
    "                       'sys_maint_reqd', 'epa_pm25_unit', 'epa_pm25_value', 'raw_concentration', 'aqi', 'category', 'site_name', 'agency_name', 'full_aqs_code', 'intl_aqs_code']]\n",
    "    \n",
    "    # Write to file\n",
    "    parquet_file = \"{0}/combined_interpolated/20{3}{1}{2:02}.parquet\".format(datafolder, month, day, yr)  \n",
    "    write(parquet_file, combined_df,compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading historical data\n",
    "def loadHistory(month, monthname, monthyear, startindex, endindex, dateint, yr):\n",
    "    # Create dataframe for existing addresses\n",
    "    address_df = pd.read_parquet(\"{}/purpleair/address_latlon.parquet\".format(datafolder))\n",
    "\n",
    "    # Since we don't have purple air data prior to 09/14, we are using the 09/14 dataset\n",
    "    # to get a list of the sensors and correponding location\n",
    "    bay_purple_df = pd.read_parquet(\"{}/purpleair/dailyfiltered/20190914.parquet\".format(datafolder))\n",
    "\n",
    "    # Get lat lon info from purple air dataset into a separate dataframe\n",
    "    bay_purple_latlon_df = bay_purple_df[['device_loc_typ', 'is_owner', 'sensor_id', 'sensor_name',  'parent_id', 'lat', 'lon', 'thingspeak_primary_id', 'thingspeak_primary_id_read_key', 'thingspeak_secondary_id', \n",
    "                                      'thingspeak_secondary_id_read_key', 'sensorhash']]\n",
    "    bay_purple_latlon_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Get noaa data for the entire month\n",
    "    bay_noaa_df = getNOAAData(month, monthyear, yr)\n",
    "\n",
    "    # Get historical epa data\n",
    "    epa_df = getEPAHistData()\n",
    "\n",
    "    for i in range(startindex, endindex):\n",
    "        try:\n",
    "            # Get thingspeak data\n",
    "            bay_ts_df = getThingspeakData(bay_purple_df, month, monthname, i, yr)\n",
    "\n",
    "            # Merge purple air data\n",
    "            bay_ts_df = mergePurpleAir(bay_purple_latlon_df, bay_ts_df, address_df, month, i, yr)\n",
    "\n",
    "            # Get noaa data\n",
    "            dly_noaa_df = getDailyNOAA(bay_noaa_df, month, i, yr)\n",
    "\n",
    "            # Get epa data\n",
    "            int_epa_df = getEPADailyData(dateint, i, month, epa_df, yr)\n",
    "\n",
    "            # Combine data and save to file\n",
    "            combineData(dly_noaa_df, int_epa_df, bay_ts_df, month, i, yr)\n",
    "        except:\n",
    "            print(month, startindex)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadHistory('05', 'may', 'May2019', 30, 32, 201905000000)\n",
    "# Errors - 06, 19,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadHistory('02', 'feb', 'Feb2019', 6, 29, 201902000000)\n",
    "# Errors - 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadHistory('04', 'apr', 'Apr2019', 21, 31, 201904000000, '19')\n",
    "# NEED TO REPROCESS WHOLE MONTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT OF DATA AT FIELD 8\n",
      "['93210KOVE', 'OVE20181212002505212/12/18', '00:25:31', '5-MIN', 'KOVE', '120825Z', 'AUTO', '090090056']\n",
      "For 01, Of the 593 requests, 377 errored out.\n",
      "Of the 593 requests, 363 errored out.\n",
      "For 02, Of the 593 requests, 372 errored out.\n",
      "Of the 593 requests, 353 errored out.\n",
      "For 03, Of the 593 requests, 372 errored out.\n",
      "Of the 593 requests, 353 errored out.\n",
      "For 04, Of the 593 requests, 376 errored out.\n",
      "Of the 593 requests, 355 errored out.\n",
      "For 05, Of the 593 requests, 361 errored out.\n",
      "Of the 593 requests, 337 errored out.\n",
      "For 06, Of the 593 requests, 345 errored out.\n",
      "Of the 593 requests, 320 errored out.\n",
      "For 07, Of the 593 requests, 356 errored out.\n",
      "Of the 593 requests, 331 errored out.\n",
      "For 08, Of the 593 requests, 362 errored out.\n",
      "Of the 593 requests, 337 errored out.\n",
      "For 09, Of the 593 requests, 356 errored out.\n",
      "Of the 593 requests, 331 errored out.\n",
      "For 10, Of the 593 requests, 355 errored out.\n",
      "Of the 593 requests, 331 errored out.\n",
      "For 11, Of the 593 requests, 331 errored out.\n",
      "Of the 593 requests, 303 errored out.\n",
      "For 12, Of the 593 requests, 334 errored out.\n",
      "Of the 593 requests, 307 errored out.\n",
      "For 13, Of the 593 requests, 333 errored out.\n",
      "Of the 593 requests, 301 errored out.\n",
      "For 14, Of the 593 requests, 312 errored out.\n",
      "Of the 593 requests, 283 errored out.\n",
      "For 15, Of the 593 requests, 349 errored out.\n",
      "Of the 593 requests, 323 errored out.\n",
      "For 16, Of the 593 requests, 346 errored out.\n",
      "Of the 593 requests, 317 errored out.\n",
      "For 17, Of the 593 requests, 339 errored out.\n",
      "Of the 593 requests, 307 errored out.\n",
      "For 18, Of the 593 requests, 339 errored out.\n",
      "Of the 593 requests, 307 errored out.\n",
      "For 19, Of the 593 requests, 328 errored out.\n",
      "Of the 593 requests, 293 errored out.\n",
      "For 20, Of the 593 requests, 335 errored out.\n",
      "Of the 593 requests, 299 errored out.\n",
      "For 21, Of the 593 requests, 330 errored out.\n",
      "Of the 593 requests, 295 errored out.\n",
      "For 22, Of the 593 requests, 329 errored out.\n",
      "Of the 593 requests, 295 errored out.\n",
      "For 23, Of the 593 requests, 333 errored out.\n",
      "Of the 593 requests, 301 errored out.\n",
      "For 24, Of the 593 requests, 331 errored out.\n",
      "Of the 593 requests, 297 errored out.\n",
      "For 25, Of the 593 requests, 327 errored out.\n",
      "Of the 593 requests, 291 errored out.\n",
      "For 26, Of the 593 requests, 319 errored out.\n",
      "Of the 593 requests, 280 errored out.\n",
      "For 27, Of the 593 requests, 309 errored out.\n",
      "Of the 593 requests, 271 errored out.\n",
      "For 28, Of the 593 requests, 303 errored out.\n",
      "Of the 593 requests, 265 errored out.\n",
      "For 29, Of the 593 requests, 294 errored out.\n",
      "Of the 593 requests, 253 errored out.\n",
      "For 30, Of the 593 requests, 296 errored out.\n",
      "Of the 593 requests, 255 errored out.\n",
      "For 31, Of the 593 requests, 285 errored out.\n",
      "Of the 593 requests, 245 errored out.\n"
     ]
    }
   ],
   "source": [
    "loadHistory('12', 'dec', 'Dec2018', 1, 32, 201812000000, '18')\n",
    "# Errors - 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT OF DATA AT FIELD 8\n",
      "['93210KOVE', 'OVE20180929001005709/29/18', '00:10:31', '5-MIN', 'KOVE', '290810Z', 'AUTO', '15016T01610094']\n",
      "For 01, Of the 593 requests, 528 errored out.\n",
      "Of the 593 requests, 525 errored out.\n",
      "For 02, Of the 593 requests, 531 errored out.\n",
      "Of the 593 requests, 531 errored out.\n",
      "For 03, Of the 593 requests, 529 errored out.\n",
      "Of the 593 requests, 529 errored out.\n",
      "For 04, Of the 593 requests, 528 errored out.\n",
      "Of the 593 requests, 527 errored out.\n",
      "For 05, Of the 593 requests, 529 errored out.\n",
      "Of the 593 requests, 529 errored out.\n",
      "For 06, Of the 593 requests, 531 errored out.\n",
      "Of the 593 requests, 531 errored out.\n",
      "For 07, Of the 593 requests, 529 errored out.\n",
      "Of the 593 requests, 529 errored out.\n",
      "For 08, Of the 593 requests, 532 errored out.\n",
      "Of the 593 requests, 531 errored out.\n",
      "For 09, Of the 593 requests, 532 errored out.\n",
      "Of the 593 requests, 531 errored out.\n",
      "For 10, Of the 593 requests, 530 errored out.\n",
      "Of the 593 requests, 529 errored out.\n",
      "For 11, Of the 593 requests, 530 errored out.\n",
      "Of the 593 requests, 529 errored out.\n",
      "For 12, Of the 593 requests, 528 errored out.\n",
      "Of the 593 requests, 527 errored out.\n",
      "For 13, Of the 593 requests, 524 errored out.\n",
      "Of the 593 requests, 523 errored out.\n",
      "For 14, Of the 593 requests, 524 errored out.\n",
      "Of the 593 requests, 523 errored out.\n",
      "For 15, Of the 593 requests, 524 errored out.\n",
      "Of the 593 requests, 523 errored out.\n",
      "For 16, Of the 593 requests, 522 errored out.\n",
      "Of the 593 requests, 521 errored out.\n",
      "For 17, Of the 593 requests, 519 errored out.\n",
      "Of the 593 requests, 517 errored out.\n",
      "For 18, Of the 593 requests, 521 errored out.\n",
      "Of the 593 requests, 519 errored out.\n",
      "For 19, Of the 593 requests, 509 errored out.\n",
      "Of the 593 requests, 507 errored out.\n",
      "For 20, Of the 593 requests, 521 errored out.\n",
      "Of the 593 requests, 519 errored out.\n",
      "For 21, Of the 593 requests, 519 errored out.\n",
      "Of the 593 requests, 517 errored out.\n",
      "For 22, Of the 593 requests, 519 errored out.\n",
      "Of the 593 requests, 517 errored out.\n",
      "For 23, Of the 593 requests, 517 errored out.\n",
      "Of the 593 requests, 515 errored out.\n",
      "For 24, Of the 593 requests, 517 errored out.\n",
      "Of the 593 requests, 515 errored out.\n",
      "For 25, Of the 593 requests, 509 errored out.\n",
      "Of the 593 requests, 507 errored out.\n",
      "For 26, Of the 593 requests, 517 errored out.\n",
      "Of the 593 requests, 515 errored out.\n",
      "For 27, Of the 593 requests, 516 errored out.\n",
      "Of the 593 requests, 513 errored out.\n",
      "For 28, Of the 593 requests, 517 errored out.\n",
      "Of the 593 requests, 515 errored out.\n",
      "For 29, Of the 593 requests, 511 errored out.\n",
      "Of the 593 requests, 509 errored out.\n",
      "For 30, Of the 593 requests, 515 errored out.\n",
      "Of the 593 requests, 513 errored out.\n"
     ]
    }
   ],
   "source": [
    "loadHistory('09', 'sep', 'Sep2018', 1, 31, 201809000000, '18')\n",
    "# Errors - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
