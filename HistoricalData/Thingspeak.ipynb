{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of PurpleAir's data\n",
    "[PurpleAir](http://www.purpleair.com) sells low-cost air quality sensors that feed data to [real-time maps of PM2.5 pollution](https://www.purpleair.com/map?#11/37.789/-122.2048).   \n",
    "This data will be used for a UC Berkeley capstone project [summarized here](https://docs.google.com/document/d/1NjCpqNd7rDnD6VOExVktGtquRzs21hpwZ8HhLQpYLO8/edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime, time\n",
    "from dateutil import tz\n",
    "import ast\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "import gmplot\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "from fastparquet import ParquetFile, write\n",
    "\n",
    "import urllib3\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "https = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to specify the paths for the data folder in your local machines\n",
    "# Use the variable 'datafolder' to specify the path\n",
    "# Comment out all the data paths except your own\n",
    "# Purple Air data ia assumed to be in a subfolder called 'purpleair' \n",
    "# For example, if the base data folder is '/users/data', purpleair data should be in '/users/data/purpleair'\n",
    "\n",
    "# Angshuman's local path\n",
    "datafolder = \"/Users/apaul2/Documents/_Common/capstone/Project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHashKey(row, col1, col2):\n",
    "    \n",
    "    str_col1 = row[col1]\n",
    "    \n",
    "    str_col2 = row[col2]\n",
    "        \n",
    "    return hash(str_col1 + str_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from sensor 2\n",
    "def genTS2DF(sensordf, startday):\n",
    "    ts_s_df = pd.DataFrame(columns=['created_at', '0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0', 'pm10_0','sensorhash'])\n",
    "    count, errCount = 0, 0\n",
    "\n",
    "    for ind, val in sensordf.iterrows():\n",
    "        qrystr = \"https://api.thingspeak.com/channels/{0}/feeds.json?api_key={0}&start=2019-09-{2}%2000:00:00&end=2019-09-{2}%2023:59:59& \\\n",
    "                    timezone=America/Los_Angeles&timescale=10\".format(val['thingspeak_secondary_id'], val['thingspeak_secondary_id_read_key'], startday)\n",
    "#         print(qrystr)\n",
    "        try:\n",
    "            count += 1\n",
    "            r = https.request('GET',qrystr)\n",
    "            if r.status == 200:\n",
    "                j = json.loads(r.data.decode('utf-8'))\n",
    "                df = pd.DataFrame(j['feeds'])\n",
    "                df.columns=['created_at', '0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0', 'pm10_0']\n",
    "                df['sensorhash'] = val['sensorhash']\n",
    "                ts_s_df = pd.concat([ts_s_df,df],ignore_index=True)\n",
    "        except Exception as e:\n",
    "            errCount += 1\n",
    "            continue\n",
    "    print(\"For {}, Of the {} requests, {} errored out.\".format(startday, count, errCount))\n",
    "    \n",
    "    # Add a key column based on time\n",
    "    # This along with the sensorhash column will be used to join the two sensor datasets\n",
    "    ts_s_df['created'] = ts_s_df['created_at'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y%m%d%H%M\"))\n",
    "    \n",
    "    return ts_s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from sensor 1\n",
    "def genTS1DF(sensordf, startday):\n",
    "    ts_p_df = pd.DataFrame(columns=['created_at', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime', 'rssi', 'temperature', 'humidity', 'pm2_5_cf_1','sensorhash'])\n",
    "    count, errCount = 0, 0\n",
    "\n",
    "    for ind, val in sensordf.iterrows():\n",
    "        qrystr = \"https://api.thingspeak.com/channels/{0}/feeds.json?api_key={1}&start=2019-09-{2}%2000:00:00&end=2019-09-{2}%2023:59:59& \\\n",
    "                    timezone=America/Los_Angeles&timescale=10\".format(val['thingspeak_primary_id'], val['thingspeak_primary_id_read_key'], startday)\n",
    "#         print(qrystr)\n",
    "        try:\n",
    "            count += 1\n",
    "            r = https.request('GET',qrystr)\n",
    "            if r.status == 200:\n",
    "                j = json.loads(r.data.decode('utf-8'))\n",
    "                df = pd.DataFrame(j['feeds'])\n",
    "                df.columns=['created_at', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime', 'rssi', 'temperature', 'humidity', 'pm2_5_cf_1']\n",
    "                df['sensorhash'] = val['sensorhash']\n",
    "                ts_p_df = pd.concat([ts_p_df,df],ignore_index=True)\n",
    "        except Exception as e:\n",
    "            errCount += 1\n",
    "            continue\n",
    "    print(\"Of the {} requests, {} errored out.\".format(count, errCount))\n",
    "    \n",
    "    # Add a key column based on time\n",
    "    # This along with the sensorhash column will be used to join the two sensor datasets\n",
    "    ts_p_df['created'] = ts_p_df['created_at'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y%m%d%H%M\"))\n",
    "    \n",
    "    return ts_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 01, Of the 593 requests, 102 errored out.\n",
      "Of the 593 requests, 36 errored out.\n",
      "For 02, Of the 593 requests, 102 errored out.\n",
      "Of the 593 requests, 37 errored out.\n",
      "For 03, Of the 593 requests, 103 errored out.\n",
      "Of the 593 requests, 37 errored out.\n",
      "For 04, Of the 593 requests, 101 errored out.\n",
      "Of the 593 requests, 34 errored out.\n",
      "For 05, Of the 593 requests, 98 errored out.\n",
      "Of the 593 requests, 30 errored out.\n",
      "For 06, Of the 593 requests, 97 errored out.\n",
      "Of the 593 requests, 29 errored out.\n",
      "For 07, Of the 593 requests, 93 errored out.\n",
      "Of the 593 requests, 26 errored out.\n",
      "For 08, Of the 593 requests, 91 errored out.\n",
      "Of the 593 requests, 21 errored out.\n",
      "For 09, Of the 593 requests, 89 errored out.\n",
      "Of the 593 requests, 20 errored out.\n",
      "For 10, Of the 593 requests, 84 errored out.\n",
      "Of the 593 requests, 15 errored out.\n",
      "For 11, Of the 593 requests, 85 errored out.\n",
      "Of the 593 requests, 16 errored out.\n",
      "For 12, Of the 593 requests, 79 errored out.\n",
      "Of the 593 requests, 10 errored out.\n",
      "For 13, Of the 593 requests, 85 errored out.\n",
      "Of the 593 requests, 16 errored out.\n",
      "For 14, Of the 593 requests, 72 errored out.\n",
      "Of the 593 requests, 2 errored out.\n"
     ]
    }
   ],
   "source": [
    "days_list = ['15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30']\n",
    "# days_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n",
    "# bayarea_purple_df = pd.read_parquet(\"{}/purpleair/dailyfiltered/20190914.parquet\".format(datafolder))\n",
    "\n",
    "for i in range(len(days_list)):\n",
    "    bayarea_purple_df = pd.read_parquet(\"{}/purpleair/dailyfiltered/201909{}.parquet\".format(datafolder,days_list[i]))\n",
    "    bay_pa_thingspeak_df = bayarea_purple_df[['sensorhash', 'thingspeak_primary_id','thingspeak_primary_id_read_key',\n",
    "                                               'thingspeak_secondary_id','thingspeak_secondary_id_read_key']]\n",
    "    bay_pa_thingspeak_df.drop_duplicates(inplace=True)\n",
    "    bay_pa_thingspeak_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    ts_s_df = genTS2DF(bay_pa_thingspeak_df, days_list[i])\n",
    "    ts_p_df = genTS1DF(bay_pa_thingspeak_df, days_list[i])\n",
    "    # Merge data from the two sensors\n",
    "    # Only keep records having particle data\n",
    "    bay_ts_df = pd.merge(ts_s_df, ts_p_df,  how='left', left_on=['sensorhash','created'], right_on=['sensorhash','created'])\n",
    "    bay_ts_df.drop(['created_at_y'], axis=1, inplace=True)\n",
    "    \n",
    "    # Write to file\n",
    "    parquet_file = \"{}/thingspeak/thingspeak_sep{}.parquet\".format(datafolder, days_list[i])\n",
    "    write(parquet_file, bay_ts_df,compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pd.read_parquet(\"{}/thingspeak/thingspeak_sep30.parquet\".format(datafolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['201909300000', '201909300010', '201909300020', '201909300030',\n",
       "       '201909300040', '201909300050', '201909300100', '201909300110',\n",
       "       '201909300120', '201909300130', '201909300140', '201909300150',\n",
       "       '201909300200', '201909300210', '201909300220', '201909300230',\n",
       "       '201909300240', '201909300250', '201909300300', '201909300310',\n",
       "       '201909300320', '201909300330', '201909300340', '201909300350',\n",
       "       '201909300400', '201909300410', '201909300420', '201909300430',\n",
       "       '201909300440', '201909300450', '201909300500', '201909300510',\n",
       "       '201909300520', '201909300530', '201909300540', '201909300550',\n",
       "       '201909300600', '201909300610', '201909300620', '201909300630',\n",
       "       '201909300640', '201909300650', '201909300700', '201909300710',\n",
       "       '201909300720', '201909300730', '201909300740', '201909300750',\n",
       "       '201909300800', '201909300810', '201909300820', '201909300830',\n",
       "       '201909300840', '201909300850', '201909300900', '201909300910',\n",
       "       '201909300920', '201909300930', '201909300940', '201909300950',\n",
       "       '201909301000', '201909301010', '201909301020', '201909301030',\n",
       "       '201909301040', '201909301050', '201909301100', '201909301110',\n",
       "       '201909301120', '201909301130', '201909301140', '201909301150',\n",
       "       '201909301200', '201909301210', '201909301220', '201909301230',\n",
       "       '201909301240', '201909301250', '201909301300', '201909301310',\n",
       "       '201909301320', '201909301330', '201909301340', '201909301350',\n",
       "       '201909301400', '201909301410', '201909301420', '201909301430',\n",
       "       '201909301440', '201909301450', '201909301500', '201909301510',\n",
       "       '201909301520', '201909301530', '201909301540', '201909301550',\n",
       "       '201909301600', '201909301610', '201909301620', '201909301630',\n",
       "       '201909301640', '201909301650', '201909301700', '201909301710',\n",
       "       '201909301720', '201909301730', '201909301740', '201909301750',\n",
       "       '201909301800', '201909301810', '201909301820', '201909301830',\n",
       "       '201909301840', '201909301850', '201909301900', '201909301910',\n",
       "       '201909301920', '201909301930', '201909301940', '201909301950',\n",
       "       '201909302000', '201909302010', '201909302020', '201909302030',\n",
       "       '201909302040', '201909302050', '201909302100', '201909302110',\n",
       "       '201909302120', '201909302130', '201909302140', '201909302150',\n",
       "       '201909302200', '201909302210', '201909302220', '201909302230',\n",
       "       '201909302240', '201909302250', '201909302300', '201909302310',\n",
       "       '201909302320', '201909302330', '201909302340', '201909302350'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.created.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
