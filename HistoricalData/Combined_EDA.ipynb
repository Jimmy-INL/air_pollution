{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np \n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import ast\n",
    "from fastparquet import ParquetFile, write\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to specify the paths for the data folder in your local machines\n",
    "# Use the variable 'datafolder' to specify the path\n",
    "# Comment out all the data paths except your own\n",
    "# Purple Air data ia assumed to be in a subfolder called 'purpleair' \n",
    "# NOAA data ia assumed to be in a subfolder called 'noaa' \n",
    "# For example, if the base data folder is '/users/data', purpleair data should be in '/users/data/purpleair'\n",
    "\n",
    "# Angshuman's local path\n",
    "datafolder = \"/Users/apaul2/Documents/_Common/capstone/Project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine PurpleAir and NOAA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHashKey(row):\n",
    "    if np.isnan(row['lat']):\n",
    "        str_lat = ''\n",
    "    else:\n",
    "        str_lat = str(row['lat'])\n",
    "        \n",
    "        \n",
    "    if np.isnan(row['lon']):\n",
    "        str_lon = ''\n",
    "    else:\n",
    "        str_lon = str(row['lon'])\n",
    "        \n",
    "    return hash(str_lat + str_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,31):\n",
    "    # Read from noaa data that was stored earlier\n",
    "    noaa_df = pd.read_parquet(\"{}/noaa/daily/asos_201909{:02}.parquet\".format(datafolder, i))\n",
    "    \n",
    "    # Read epa data from file\n",
    "    epa_df = pd.read_parquet(\"{}/ambient/daily/epa_201909{:02}.parquet\".format(datafolder, i))\n",
    "    epa_df['createdhr'] = epa_df['created'].apply(lambda x: int(str(x)[:-2]))  # date key at hour level as the data is hourly\n",
    "    \n",
    "    # Read purple air data from file\n",
    "    bay_ts_df = pd.read_parquet(\"{}/pa_ts/201909{:02}.parquet\".format(datafolder, i))\n",
    "    bay_ts_df['createdhr'] = bay_ts_df['created'].apply(lambda x: int(str(x)[:-2]))  # date key at hour level to join with hourly epa data\n",
    "    \n",
    "    # Add lat-lon based hashes to noaa and purple air dataframes\n",
    "    bay_ts_df['tslatlonhash'] = bay_ts_df.apply (lambda row: createHashKey(row), axis=1)\n",
    "    noaa_df['asoslatlonhash'] = noaa_df.apply (lambda row: createHashKey(row), axis=1)\n",
    "    epa_df['epalatlonhash'] = epa_df.apply (lambda row: createHashKey(row), axis=1)\n",
    "    \n",
    "    # Keep only the asos columns needed to determine the lat-lon mapping\n",
    "    noaa_latlon_df = noaa_df[['asoslatlonhash','lat','lon']]\n",
    "    noaa_latlon_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Keep only the epa columns needed to determine the lat-lon mapping\n",
    "    epa_latlon_df = epa_df[['epalatlonhash','lat','lon']]\n",
    "    epa_latlon_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Keep only the purple air columns needed to determine the lat-lon mapping\n",
    "    usa_purple_latlon_df = bay_ts_df[['tslatlonhash','lat','lon']]\n",
    "    usa_purple_latlon_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    noaa_latlon_df.set_index('asoslatlonhash', inplace=True)\n",
    "    usa_purple_latlon_df.set_index('tslatlonhash', inplace=True)\n",
    "    epa_latlon_df.set_index('epalatlonhash', inplace=True)\n",
    "    \n",
    "    # Find the closest asos lat-lon mapping corresponding to the purple air records\n",
    "    closest_asos_points = {}\n",
    "    for name, point in usa_purple_latlon_df.iterrows():\n",
    "    #     print(name, point)\n",
    "    #     break\n",
    "        distances = (((noaa_latlon_df - point) ** 2).sum(axis=1)**.5)\n",
    "        closest_asos_points[name] = distances.sort_values().index[0]\n",
    "\n",
    "    # Create dataframe from lat-lon mapping\n",
    "    asoslatlonmap_df = pd.DataFrame(list(closest_asos_points.items()), columns=['tslatlonhash','asoslatlonhash'])\n",
    "    \n",
    "    # Find the closest asos lat-lon mapping corresponding to the purple air records\n",
    "    closest_epa_points = {}\n",
    "    for name, point in usa_purple_latlon_df.iterrows():\n",
    "    #     print(name, point)\n",
    "    #     break\n",
    "        distances = (((epa_latlon_df - point) ** 2).sum(axis=1)**.5)\n",
    "        closest_epa_points[name] = distances.sort_values().index[0]\n",
    "\n",
    "    # Create dataframe from lat-lon mapping\n",
    "    epalatlonmap_df = pd.DataFrame(list(closest_epa_points.items()), columns=['tslatlonhash','epalatlonhash'])\n",
    "    \n",
    "    # Merge purple air data to lat-lon mappings first and then \n",
    "    # merge the resulting dataframe to asos and epa dataframes\n",
    "    merged_df = pd.merge(bay_ts_df, asoslatlonmap_df, on='tslatlonhash')\n",
    "    merged_df = pd.merge(merged_df, epalatlonmap_df, on='tslatlonhash')\n",
    "    \n",
    "    # Drop common and unwanted columns from noaa and epa dataframes\n",
    "    noaa_df.drop(['lat','lon'], axis=1, inplace=True)\n",
    "    epa_df.drop(['lat','lon'], axis=1, inplace=True)\n",
    "    \n",
    "    # Combine asos data\n",
    "    combined_df = pd.merge(merged_df, noaa_df,  how='left', left_on=['asoslatlonhash', 'created'], right_on=['asoslatlonhash', 'datetime'])\n",
    "\n",
    "    # Combine epa data\n",
    "    combined_df = pd.merge(combined_df, epa_df,  how='left', left_on=['epalatlonhash', 'createdhr'], right_on=['epalatlonhash', 'createdhr'])\n",
    "\n",
    "    # # Drop unwanted columns\n",
    "    combined_df.drop(['tslatlonhash', 'asoslatlonhash', 'epalatlonhash', 'rec_length','num_fields', 'datetime', 'utc', 'parameter', 'createdhr','created_y'], axis=1, inplace=True)\n",
    "    \n",
    "    combined_df.columns = ['0_3um', '0_5um', '1_0um', '2_5um', '5_0um', '10_0um', 'pm1_0','pm10_0', 'created', 'pm1_0_atm', 'pm2_5_atm', 'pm10_0_atm', 'uptime','rssi', \n",
    "                       'temperature', 'humidity', 'pm2_5_cf_1', 'device_loc_typ', 'is_owner', 'sensor_id', 'sensor_name', 'parent_id','lat', 'lon',  'thingspeak_primary_id', \n",
    "                       'thingspeak_primary_id_read_key', 'thingspeak_secondary_id', 'thingspeak_secondary_id_read_key', 'a_h', 'high_reading_flag', 'hidden',\n",
    "                       'city', 'county', 'zipcode', 'created_at', 'year', 'month', 'day', 'hour', 'minute', 'wban_number', 'call_sign', 'call_sign2', 'interval', \n",
    "                       'call_sign3', 'zulu_time', 'report_modifier', 'wind_data', 'wind_direction', 'wind_speed', 'gusts', 'gust_speed', 'variable_winds', 'variable_wind_info', \n",
    "                       'sys_maint_reqd', 'epa_pm25_unit', 'epa_pm25_value', 'raw_concentration', 'aqi', 'category', 'site_name', 'agency_name', 'full_aqs_code', 'intl_aqs_code']\n",
    "    \n",
    "    # Write to file\n",
    "    parquet_file = \"{}/combined/201909{:02}.parquet\".format(datafolder, i)\n",
    "    write(parquet_file, combined_df,compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pd.read_parquet(\"{}/combined/20190930.parquet\".format(datafolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201909300000, 201909300010, 201909300020, 201909300030,\n",
       "       201909300040, 201909300050, 201909300100, 201909300110,\n",
       "       201909300120, 201909300130, 201909300140, 201909300150,\n",
       "       201909300200, 201909300210, 201909300220, 201909300230,\n",
       "       201909300240, 201909300250, 201909300300, 201909300310,\n",
       "       201909300320, 201909300330, 201909300340, 201909300350,\n",
       "       201909300400, 201909300410, 201909300420, 201909300430,\n",
       "       201909300440, 201909300450, 201909300500, 201909300510,\n",
       "       201909300520, 201909300530, 201909300540, 201909300550,\n",
       "       201909300600, 201909300610, 201909300620, 201909300630,\n",
       "       201909300640, 201909300650, 201909300700, 201909300710,\n",
       "       201909300720, 201909300730, 201909300740, 201909300750,\n",
       "       201909300800, 201909300810, 201909300820, 201909300830,\n",
       "       201909300840, 201909300850, 201909300900, 201909300910,\n",
       "       201909300920, 201909300930, 201909300940, 201909300950,\n",
       "       201909301000, 201909301010, 201909301020, 201909301030,\n",
       "       201909301040, 201909301050, 201909301100, 201909301110,\n",
       "       201909301120, 201909301130, 201909301140, 201909301150,\n",
       "       201909301200, 201909301210, 201909301220, 201909301230,\n",
       "       201909301240, 201909301250, 201909301300, 201909301310,\n",
       "       201909301320, 201909301330, 201909301340, 201909301350,\n",
       "       201909301400, 201909301410, 201909301420, 201909301430,\n",
       "       201909301440, 201909301450, 201909301500, 201909301510,\n",
       "       201909301520, 201909301530, 201909301540, 201909301550,\n",
       "       201909301600, 201909301610, 201909301620, 201909301630,\n",
       "       201909301640, 201909301650, 201909301700, 201909301710,\n",
       "       201909301720, 201909301730, 201909301740, 201909301750,\n",
       "       201909301800, 201909301810, 201909301820, 201909301830,\n",
       "       201909301840, 201909301850, 201909301900, 201909301910,\n",
       "       201909301920, 201909301930, 201909301940, 201909301950,\n",
       "       201909302000, 201909302010, 201909302020, 201909302030,\n",
       "       201909302040, 201909302050, 201909302100, 201909302110,\n",
       "       201909302120, 201909302130, 201909302140, 201909302150,\n",
       "       201909302200, 201909302210, 201909302220, 201909302230,\n",
       "       201909302240, 201909302250, 201909302300, 201909302310,\n",
       "       201909302320, 201909302330, 201909302340, 201909302350])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.created.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
