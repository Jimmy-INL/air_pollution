{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '/opt/spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, ArrayType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import mean, array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"whatever\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_locs = pd.read_csv(\"sensor_locs_big_box.csv\").set_index(\"sensor_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>xy_</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensor_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14091</th>\n",
       "      <td>37.883620</td>\n",
       "      <td>-122.070087</td>\n",
       "      <td>(136, 170)</td>\n",
       "      <td>136</td>\n",
       "      <td>170</td>\n",
       "      <td>5099</td>\n",
       "      <td>106.826744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>38.028500</td>\n",
       "      <td>-122.030200</td>\n",
       "      <td>(145, 211)</td>\n",
       "      <td>145</td>\n",
       "      <td>211</td>\n",
       "      <td>2763</td>\n",
       "      <td>8.664356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12811</th>\n",
       "      <td>37.406370</td>\n",
       "      <td>-122.062429</td>\n",
       "      <td>(138, 35)</td>\n",
       "      <td>138</td>\n",
       "      <td>35</td>\n",
       "      <td>2755</td>\n",
       "      <td>12.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>37.787307</td>\n",
       "      <td>-122.417252</td>\n",
       "      <td>(59, 142)</td>\n",
       "      <td>59</td>\n",
       "      <td>142</td>\n",
       "      <td>931</td>\n",
       "      <td>40.919197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10092</th>\n",
       "      <td>37.857566</td>\n",
       "      <td>-121.972860</td>\n",
       "      <td>(158, 162)</td>\n",
       "      <td>158</td>\n",
       "      <td>162</td>\n",
       "      <td>4230</td>\n",
       "      <td>195.418274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 lat         lon         xy_    x    y  ndvi   elevation\n",
       "sensor_id                                                               \n",
       "14091      37.883620 -122.070087  (136, 170)  136  170  5099  106.826744\n",
       "8988       38.028500 -122.030200  (145, 211)  145  211  2763    8.664356\n",
       "12811      37.406370 -122.062429   (138, 35)  138   35  2755   12.595800\n",
       "4770       37.787307 -122.417252   (59, 142)   59  142   931   40.919197\n",
       "10092      37.857566 -121.972860  (158, 162)  158  162  4230  195.418274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_locs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lookup = sensor_locs.T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['201908_bigger_3.parquet',\n",
       " '201907_bigger_3.parquet',\n",
       " '201906_bigger_3.parquet',\n",
       " '201905_bigger_3.parquet',\n",
       " '201904_bigger_3.parquet',\n",
       " '201903_bigger_3.parquet',\n",
       " '201902_bigger_3.parquet',\n",
       " '201901_bigger_3.parquet',\n",
       " '201812_bigger_3.parquet',\n",
       " '201811_bigger_3.parquet',\n",
       " '201810_bigger_3.parquet',\n",
       " '201809_bigger_3.parquet']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob.glob(\"*bigger_3.parquet\"), reverse=True)[-12:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old code to sparkify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#spark.udf.register(\"getNeighbors\", get_neighbors_space_time, ArrayType(DoubleType()))\n",
    "lookup_xy = udf(lambda s, col: id_lookup[s][col], IntegerType())\n",
    "lookup_other = udf(lambda s, col: id_lookup[s][col], DoubleType())\n",
    "ts = udf(lambda x: int(datetime.datetime.strptime(x, \"%Y/%m/%dT%H:%M\").timestamp()), IntegerType())\n",
    "ts_id = udf(lambda arr: \"_\".join([str(n) for n in arr]), StringType())\n",
    "n_neighbors = udf(lambda ns: (np.array(ns) > 0).sum(), IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "finished_files = []\n",
    "\n",
    "def full_process(n, filename):\n",
    "    df = spark.read.parquet(filename)\n",
    "\n",
    "    df = df.withColumn('x', lookup_xy(df.sensor_id.cast(IntegerType()), F.lit('x'))) \\\n",
    "        .withColumn('y', lookup_xy(df.sensor_id.cast(IntegerType()), F.lit('y'))) \\\n",
    "        .withColumn('ndvi', lookup_xy(df.sensor_id.cast(IntegerType()), F.lit('ndvi'))) \\\n",
    "        .withColumn('elevation', lookup_other(df.sensor_id.cast(IntegerType()), F.lit('elevation'))) \\\n",
    "        .withColumn('ts_', ts(df.created_at))\n",
    "\n",
    "    averages = df.groupBy(\"ts_\").mean()\n",
    "    df_with_avgs = df.join(averages, on=\"ts_\")\n",
    "    imputed_df = df_with_avgs.withColumn(\"imputed_hum\", F.coalesce(\"humidity\", \"avg(humidity)\")) \\\n",
    "        .withColumn(\"imputed_temperature\", F.coalesce(\"temperature\", \"avg(temperature)\")) \\\n",
    "        .withColumn(\"imputed_epa_pm25_value\", F.coalesce(\"epa_pm25_value\", \"avg(epa_pm25_value)\")) \\\n",
    "        .withColumn(\"time_space_id\", ts_id(array('ts_', 'x', 'y')))\n",
    "\n",
    "    ts_averages = imputed_df.groupBy(['time_space_id']) \\\n",
    "                    .agg({'2_5um':'mean', 'sensor_id':'count'})\n",
    "\n",
    "    ts_average_dict = ts_averages.toPandas().set_index('time_space_id').T.to_dict()\n",
    "    \n",
    "    def get_neighbors_space_time(ts_, x, y, pm):\n",
    "        \"\"\"\n",
    "        Inputs: single observation, a training dataframe, and a time delta\n",
    "        Outputs: vector of length 25 corresponding to surrounding neighbor observations\n",
    "        \n",
    "        In case you are wondering, I have to redfine this every loop because of the way\n",
    "        the ts_avg_dict is broadcast\n",
    "        \n",
    "        \"\"\"\n",
    "        ts_ = int(ts_)\n",
    "        x = int(x)\n",
    "        y = int(y)\n",
    "\n",
    "\n",
    "        neighbors = np.zeros((25))\n",
    "\n",
    "        c = 0\n",
    "        for i in range(-2,3):\n",
    "            for j in range(-2,3):\n",
    "                ts_id_ = f\"{ts_}_{x+i}_{y+j}\"\n",
    "                if i == 0 and j == 0:\n",
    "                    if ts_average_dict[ts_id_]['count(sensor_id)'] > 1:\n",
    "                        n_s = ts_average_dict[ts_id_]['count(sensor_id)']\n",
    "                        avg = ts_average_dict[ts_id_]['avg(2_5um)']\n",
    "                        # remove the sensor itself from consideration\n",
    "                        neighbors[c] = ((n_s*avg) - pm)/(n_s-1)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if ts_id_ in ts_average_dict:\n",
    "                        neighbors[c] = ts_average_dict[ts_id_]['avg(2_5um)']       \n",
    "                c += 1\n",
    "\n",
    "        return neighbors.tolist()\n",
    "    get_neighbors = udf(lambda arr: get_neighbors_space_time(*arr), ArrayType(DoubleType()))    \n",
    "    \n",
    "    df_w_neighbors = imputed_df \\\n",
    "        .withColumn('neighbors', get_neighbors(array('ts_', 'x', 'y', '2_5um')))\n",
    "\n",
    "\n",
    "    \n",
    "    cols_to_save = ['2_5um', 'imputed_epa_pm25_value', 'imputed_hum', \n",
    "                    'imputed_temperature', 'wind_x', 'wind_y', 'ndvi',\n",
    "                    'elevation', 'neighbors'] # + neighbor_cols\n",
    "\n",
    "    if n == 0:\n",
    "        df_w_neighbors \\\n",
    "            .select(cols_to_save) \\\n",
    "            .write.parquet('big_processed_4.parquet')\n",
    "        finished_files.append(filename)\n",
    "    else:\n",
    "        df_w_neighbors \\\n",
    "            .select(cols_to_save) \\\n",
    "            .write.mode(\"append\").parquet('big_processed_4.parquet')\n",
    "        finished_files.append(filename)\n",
    "    print(filename, \"done\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201908_bigger_3.parquet done\n",
      "201907_bigger_3.parquet done\n",
      "201906_bigger_3.parquet done\n",
      "201905_bigger_3.parquet done\n",
      "201904_bigger_3.parquet done\n",
      "201903_bigger_3.parquet done\n",
      "201902_bigger_3.parquet done\n",
      "201901_bigger_3.parquet done\n",
      "201812_bigger_3.parquet done\n",
      "201811_bigger_3.parquet done\n",
      "201810_bigger_3.parquet done\n",
      "201809_bigger_3.parquet done\n",
      "CPU times: user 11min 34s, sys: 9.72 s, total: 11min 43s\n",
      "Wall time: 21min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for n, filename in enumerate(sorted(glob.glob(\"*bigger_3.parquet\"), reverse=True)[-12:]):\n",
    "    full_process(n, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('big_processed_4.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 2_5um: double (nullable = true)\n",
      " |-- imputed_epa_pm25_value: double (nullable = true)\n",
      " |-- imputed_hum: double (nullable = true)\n",
      " |-- imputed_temperature: double (nullable = true)\n",
      " |-- wind_x: double (nullable = true)\n",
      " |-- wind_y: double (nullable = true)\n",
      " |-- ndvi: integer (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- neighbors: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
